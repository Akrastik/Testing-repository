{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral.rs Python API Cookbook\n",
    "\n",
    "The Mistral.rs API is broken into 2 parts: loading and running. We provide several loader classes which can create a `Runner` class.\n",
    "Lets look at an example of loading a Mistral GGUF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralrs import MistralLoader, QuantizedLoader, ChatCompletionRequest\n",
    "\n",
    "loader = QuantizedLoader(\n",
    "    MistralLoader,\n",
    "    is_gguf=True,\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    no_kv_cache=False,\n",
    "    repeat_last_n=64,\n",
    "    quantized_model_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    quantized_filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    ")\n",
    "runner = loader.load()\n",
    "res = runner.send_chat_completion_request(\n",
    "    ChatCompletionRequest(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a story about the Rust type system.\"}\n",
    "        ],\n",
    "        max_tokens=256,\n",
    "        presence_penalty=1.0,\n",
    "        top_p=0.1,\n",
    "        temperature=0.1,\n",
    "    )\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets walk through this code.\n",
    "```python\n",
    "from mistralrs import MistralLoader, QuantizedLoader, ChatCompletionRequest\n",
    "```\n",
    "\n",
    "This imports the requires classes for our example. The `QuantizedLoader` needs to know which model architecture to load, and so we also import the `MistralLoader` which implements that functionality. The `ChatCompletionRequest` is an OpenAI API compatible class which allows you to send requests.\n",
    "\n",
    "```python\n",
    "loader = QuantizedLoader(\n",
    "    MistralLoader,\n",
    "    is_gguf=True,\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    no_kv_cache=False,\n",
    "    repeat_last_n=64,\n",
    "    quantized_model_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    quantized_filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    ")\n",
    "```\n",
    "\n",
    "Here, we tell the `QuantizedLoader` to get ready to load a Mistral model (because we gave it a `MistralLoader`) with the model ID and other information that we specified. It is also important to specify if the quantized model is GGUF, if it is not (i.e. it is GGML), the `is_gguf=False` should be passed.\n",
    "\n",
    "```python\n",
    "runner = loader.load()\n",
    "```\n",
    "\n",
    "This tells the `QuantizedLoader` to actually load the model. It will use a CUDA, Metal, or CPU device depending on what `features` you set during compilation: [here](https://github.com/EricLBuehler/mistral.rs?tab=readme-ov-file#supported-accelerators).\n",
    "\n",
    "```python\n",
    "res = runner.send_chat_completion_request(\n",
    "    ChatCompletionRequest(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a story about the Rust type system.\"}\n",
    "        ],\n",
    "        max_tokens=256,\n",
    "        presence_penalty=1.0,\n",
    "        top_p=0.1,\n",
    "        temperature=0.1,\n",
    "    )\n",
    ")\n",
    "print(res)\n",
    "```\n",
    "\n",
    "Now we actually send a request! We can specify the messages just like with an OpenAI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Mistral + GGUF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralrs import MistralLoader, QuantizedLoader\n",
    "\n",
    "loader = QuantizedLoader(\n",
    "    MistralLoader,\n",
    "    is_gguf=True,\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    no_kv_cache=False,\n",
    "    repeat_last_n=64,\n",
    "    quantized_model_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    quantized_filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    ")\n",
    "runner = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a plain Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralrs import MistralLoader, NormalLoader\n",
    "\n",
    "loader = NormalLoader(\n",
    "    MistralLoader,\n",
    "    is_gguf=True,\n",
    "    model_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    no_kv_cache=False,\n",
    "    repeat_last_n=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an X-LoRA Zephyr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralrs import MistralLoader, XLoraLoader\n",
    "\n",
    "loader = XLoraLoader(\n",
    "    MistralLoader,\n",
    "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    no_kv_cache=False,\n",
    "    repeat_last_n=64,\n",
    "    xlora_model_id=\"lamm-mit/x-lora\",\n",
    "    order_file=\"xlora-paper-ordering.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralrs import ChatCompletionRequest\n",
    "\n",
    "res = runner.send_chat_completion_request(\n",
    "    ChatCompletionRequest(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a story about the Rust type system.\"}\n",
    "        ],\n",
    "        max_tokens=256,\n",
    "        presence_penalty=1.0,\n",
    "        top_p=0.1,\n",
    "        temperature=0.1,\n",
    "    )\n",
    ")\n",
    "print(res)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
