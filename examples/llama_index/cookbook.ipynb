{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama index cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/EricLBuehler/mistral.rs.git\n",
    "!cd mistral.rs/mistralrs-pyo3\n",
    "!pip install maturin[patchelf]\n",
    "!maturin develop -r --features ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Source (paste )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/EricLBuehler/mistral.rs/blob/master/integrations/llama_index_integration.py\n",
    "from typing import Any, Callable, Dict, Optional, Sequence\n",
    "\n",
    "from llama_index.core.base.llms.types import (\n",
    "    ChatMessage,\n",
    "    ChatResponse,\n",
    "    ChatResponseGen,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    "    MessageRole,\n",
    ")\n",
    "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.constants import (\n",
    "    DEFAULT_CONTEXT_WINDOW,\n",
    "    DEFAULT_NUM_OUTPUTS,\n",
    "    DEFAULT_TEMPERATURE,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback\n",
    "from llama_index.core.llms.custom import CustomLLM\n",
    "from llama_index.core.base.llms.generic_utils import (\n",
    "    completion_response_to_chat_response,\n",
    ")\n",
    "from llama_index.core.types import BaseOutputParser, PydanticProgramMode\n",
    "\n",
    "from mistralrs import (\n",
    "    ChatCompletionRequest,\n",
    "    Runner,\n",
    "    Which,\n",
    "    Message,\n",
    "    Role,\n",
    ")\n",
    "\n",
    "DEFAULT_MISTRAL_RS_GGML_MODEL = (\n",
    "    \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve\"\n",
    "    \"/main/llama-2-13b-chat.ggmlv3.q4_0.bin\"\n",
    ")\n",
    "DEFAULT_MISTRAL_RS_GGUF_MODEL = (\n",
    "    \"https://huggingface.co/TheBloke/Ll ama-2-13B-chat-GGUF/resolve\"\n",
    "    \"/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    ")\n",
    "DEFAULT_TOPK = 32\n",
    "DEFAULT_TOPP = 0.1\n",
    "DEFAULT_TOP_LOGPROBS = 10\n",
    "DEFAULT_REPEAT_LAST_N = 64\n",
    "DEFAULT_MAX_SEQS = 16\n",
    "DEFAULT_PREFIX_CACHE_N = 16\n",
    "\n",
    "\n",
    "def llama_index_to_mistralrs_messages(messages: Sequence[ChatMessage]) -> list[Message]:\n",
    "    \"\"\"\n",
    "    Convert llamaindex to mistralrs messages. Raises an exception if the role is not user or assistant.\n",
    "    \"\"\"\n",
    "    messages_new = []\n",
    "    for message in messages:\n",
    "        if message.role == \"user\":\n",
    "            messages_new.append(Message(Role.User, message.content))\n",
    "        elif message.role == \"assistant\":\n",
    "            messages_new.append(Message(Role.Assistant, message.content))\n",
    "        elif message.role == \"system\":\n",
    "            messages_new.append(Message(Role.System, message.content))\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported chat role `{message.role}` for `mistralrs` automatic chat templating: supported are `user`, `assistant`, `system`. Please specify `messages_to_prompt`.\"\n",
    "            )\n",
    "    return messages_new\n",
    "\n",
    "\n",
    "class MistralRS(CustomLLM):\n",
    "    r\"\"\"MistralRS LLM.\n",
    "\n",
    "    Examples:\n",
    "        Install `mistralrs` following instructions:\n",
    "        https://github.com/EricLBuehler/mistral.rs/blob/master/mistralrs-pyo3/README.md#installation\n",
    "\n",
    "        Then `pip install llama-index-llms-mistral-rs`\n",
    "\n",
    "        This LLM provides automatic chat templating as an option. If you do not provide `messages_to_prompt`,\n",
    "        mistral.rs will automatically determine one. You can specify a JINJA chat template by passing it in\n",
    "        `model_kwargs` in the `chat_template` key.\n",
    "\n",
    "        ```python\n",
    "        from llama_index.llms.mistral_rs import MistralRS\n",
    "        from mistralrs import Which\n",
    "\n",
    "        llm = MistralRS(\n",
    "            which = Which.XLora(\n",
    "                model_id=None,  # Automatically determine from ordering file\n",
    "                tokenizer_json=None,\n",
    "                repeat_last_n=64,\n",
    "                xlora_model_id=\"lamm-mit/x-lora\"\n",
    "                order=\"xlora-paper-ordering.json\", # Make sure you copy the ordering file from `mistral.rs/orderings`\n",
    "                tgt_non_granular_index=None,\n",
    "                arch=Architecture.Mistral,\n",
    "            ),\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=256,\n",
    "            context_window=3900,\n",
    "            generate_kwargs={},\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        response = llm.complete(\"Hello, how are you?\")\n",
    "        print(str(response))\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    model_url: Optional[str] = Field(\n",
    "        description=\"The URL llama-cpp model to download and use.\"\n",
    "    )\n",
    "    model_path: Optional[str] = Field(\n",
    "        description=\"The path to the llama-cpp model to use.\"\n",
    "    )\n",
    "    temperature: float = Field(\n",
    "        default=DEFAULT_TEMPERATURE,\n",
    "        description=\"The temperature to use for sampling.\",\n",
    "        gte=0.0,\n",
    "        lte=1.0,\n",
    "    )\n",
    "    max_new_tokens: int = Field(\n",
    "        default=DEFAULT_NUM_OUTPUTS,\n",
    "        description=\"The maximum number of tokens to generate.\",\n",
    "        gt=0,\n",
    "    )\n",
    "    context_window: int = Field(\n",
    "        default=DEFAULT_CONTEXT_WINDOW,\n",
    "        description=\"The maximum number of context tokens for the model.\",\n",
    "        gt=0,\n",
    "    )\n",
    "    generate_kwargs: Dict[str, Any] = Field(\n",
    "        default_factory=dict, description=\"Kwargs used for generation.\"\n",
    "    )\n",
    "    model_kwargs: Dict[str, Any] = Field(\n",
    "        default_factory=dict, description=\"Kwargs used for model initialization.\"\n",
    "    )\n",
    "    _runner: Runner = PrivateAttr(\"Mistral.rs model runner.\")\n",
    "    _has_messages_to_prompt: bool = PrivateAttr(\"If `messages_to_prompt` is provided.\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        which: Which,\n",
    "        temperature: float = DEFAULT_TEMPERATURE,\n",
    "        max_new_tokens: int = DEFAULT_NUM_OUTPUTS,\n",
    "        context_window: int = DEFAULT_CONTEXT_WINDOW,\n",
    "        top_k: int = DEFAULT_TOPK,\n",
    "        top_p: int = DEFAULT_TOPP,\n",
    "        top_logprobs: Optional[int] = DEFAULT_TOP_LOGPROBS,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        generate_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        model_kwargs: Optional[Dict[str, Any]] = {},\n",
    "        system_prompt: Optional[str] = None,\n",
    "        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,\n",
    "        completion_to_prompt: Optional[Callable[[str], str]] = None,\n",
    "        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,\n",
    "        output_parser: Optional[BaseOutputParser] = None,\n",
    "    ) -> None:\n",
    "        generate_kwargs = generate_kwargs or {}\n",
    "        generate_kwargs.update(\n",
    "            {\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_new_tokens,\n",
    "                \"top_k\": top_k,\n",
    "                \"top_p\": top_p,\n",
    "                \"top_logprobs\": top_logprobs,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            model_path=\"local\",\n",
    "            model_url=\"local\",\n",
    "            temperature=temperature,\n",
    "            context_window=context_window,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            callback_manager=callback_manager,\n",
    "            generate_kwargs=generate_kwargs,\n",
    "            model_kwargs={},\n",
    "            verbose=True,\n",
    "            system_prompt=system_prompt,\n",
    "            messages_to_prompt=messages_to_prompt,\n",
    "            completion_to_prompt=completion_to_prompt,\n",
    "            pydantic_program_mode=pydantic_program_mode,\n",
    "            output_parser=output_parser,\n",
    "        )\n",
    "\n",
    "        self._runner = Runner(\n",
    "            which=which,\n",
    "            token_source=model_kwargs.get(\"token_source\", \"cache\"),\n",
    "            max_seqs=model_kwargs.get(\"max_seqs\", DEFAULT_MAX_SEQS),\n",
    "            prefix_cache_n=model_kwargs.get(\"prefix_cache_n\", DEFAULT_PREFIX_CACHE_N),\n",
    "            no_kv_cache=model_kwargs.get(\"no_kv_cache\", False),\n",
    "            chat_template=model_kwargs.get(\"chat_template\", None),\n",
    "        )\n",
    "        self._has_messages_to_prompt = messages_to_prompt is not None\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"MistralRS_llm\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.max_new_tokens,\n",
    "            model_name=self.model_path,\n",
    "        )\n",
    "\n",
    "    @llm_chat_callback()\n",
    "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n",
    "        if self._has_messages_to_prompt:\n",
    "            messages = self.messages_to_prompt(messages)\n",
    "        else:\n",
    "            messages = llama_index_to_mistralrs_messages(messages)\n",
    "        self.generate_kwargs.update({\"stream\": False})\n",
    "\n",
    "        request = ChatCompletionRequest(\n",
    "            messages=messages,\n",
    "            model=\"\",\n",
    "            logit_bias=None,\n",
    "            logprobs=False,\n",
    "            **self.generate_kwargs,\n",
    "        )\n",
    "\n",
    "        response = self._runner.send_chat_completion_request(request)\n",
    "        return CompletionResponse(text=response.choices[0].message.content)\n",
    "\n",
    "    @llm_chat_callback()\n",
    "    def stream_chat(\n",
    "        self, messages: Sequence[ChatMessage], **kwargs: Any\n",
    "    ) -> ChatResponseGen:\n",
    "        if self._has_messages_to_prompt:\n",
    "            messages = self.messages_to_prompt(messages)\n",
    "        else:\n",
    "            messages = llama_index_to_mistralrs_messages(messages)\n",
    "        self.generate_kwargs.update({\"stream\": True})\n",
    "\n",
    "        request = ChatCompletionRequest(\n",
    "            messages=messages,\n",
    "            model=\"\",\n",
    "            logit_bias=None,\n",
    "            logprobs=False,\n",
    "            **self.generate_kwargs,\n",
    "        )\n",
    "\n",
    "        streamer = self._runner.send_chat_completion_request(request)\n",
    "\n",
    "        def gen() -> CompletionResponseGen:\n",
    "            text = \"\"\n",
    "            for response in streamer:\n",
    "                delta = response.choices[0].delta.content\n",
    "                text += delta\n",
    "                yield ChatResponse(\n",
    "                    message=ChatMessage(\n",
    "                        role=MessageRole.ASSISTANT,\n",
    "                        content=delta,\n",
    "                    ),\n",
    "                    delta=delta,\n",
    "                )\n",
    "\n",
    "        return gen()\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(\n",
    "        self, prompt: str, formatted: bool = False, **kwargs: Any\n",
    "    ) -> CompletionResponse:\n",
    "        self.generate_kwargs.update({\"stream\": False})\n",
    "        if not formatted:\n",
    "            prompt = self.completion_to_prompt(prompt)\n",
    "\n",
    "        request = ChatCompletionRequest(\n",
    "            messages=prompt,\n",
    "            model=\"\",\n",
    "            logit_bias=None,\n",
    "            logprobs=False,\n",
    "            **self.generate_kwargs,\n",
    "        )\n",
    "        completion_response = self._runner.send_chat_completion_request(request)\n",
    "        return CompletionResponse(text=completion_response.choices[0].message.content)\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "        self, prompt: str, formatted: bool = False, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        self.generate_kwargs.update({\"stream\": True})\n",
    "        if not formatted:\n",
    "            prompt = self.completion_to_prompt(prompt)\n",
    "\n",
    "        request = ChatCompletionRequest(\n",
    "            messages=prompt,\n",
    "            model=\"\",\n",
    "            logit_bias=None,\n",
    "            logprobs=False,\n",
    "            **self.generate_kwargs,\n",
    "        )\n",
    "\n",
    "        streamer = self._runner.send_chat_completion_request(request)\n",
    "\n",
    "        def gen() -> CompletionResponseGen:\n",
    "            text = \"\"\n",
    "            for response in streamer:\n",
    "                delta = response.choices[0].delta.content\n",
    "                text += delta\n",
    "                yield CompletionResponse(delta=delta, text=text)\n",
    "\n",
    "        return gen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-LoRA Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.llms.mistral_rs import MistralRS\n",
    "from mistralrs import Which\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = MistralRS(\n",
    "    which=Which.XLoraGGUF(\n",
    "        tok_model_id=None,  # Automatically determine from ordering file\n",
    "        quantized_model_id=\"TheBloke/zephyr-7B-beta-GGUF\",\n",
    "        quantized_filename=\"zephyr-7b-beta.Q4_0.gguf\",\n",
    "        tokenizer_json=None,\n",
    "        repeat_last_n=64,\n",
    "        xlora_model_id=\"lamm-mit/x-lora\",\n",
    "        order=\"../orderings/xlora-paper-ordering.json\",\n",
    "        tgt_non_granular_index=None,\n",
    "    ),\n",
    "    max_new_tokens=4096,\n",
    "    context_window=1024 * 5,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"How do I pronounce graphene?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.llms.mistral_rs import MistralRS\n",
    "from mistralrs import Which\n",
    "import sys\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = MistralRS(\n",
    "    which=Which.GGUF(\n",
    "        tok_model_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        quantized_model_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "        quantized_filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "        tokenizer_json=None,\n",
    "        repeat_last_n=64,\n",
    "    ),\n",
    "    max_new_tokens=4096,\n",
    "    context_window=1024 * 5,\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"What are non-granular scalings?\")\n",
    "for text in response.response_gen:\n",
    "    print(text, end=\"\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
