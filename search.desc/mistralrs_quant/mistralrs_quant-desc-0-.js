searchState.loadedDescShard("mistralrs_quant", 0, "Quantized method for a quantized matmul.\nAdd a delta weight from LoRA to the weights. This should …\nConvert this layer to an ISQ-able layer if possible.\nWeight dtype and device\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nIf the quant is backed by a qmatmul.\nIf the quant is backed by a qmatmul.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nIf a quantized method, return the activation dtype.")