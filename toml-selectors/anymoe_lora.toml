[model]
model_id = "HuggingFaceH4/zephyr-7b-beta"
arch = "mistral"

[anymoe]
dataset_csv = "test.csv"
prefix = "model.layers"
mlp = "mlp"
model_ids = ["typeof/zephyr-7b-beta-lora"]

[anymoe.config]
hidden_size = 4096

[anymoe.config.expert_type.lora_adapter]
rank = 64
alpha = 16
target_modules = ["gate_proj"]
